# Adaptive Aggregation Networks for Class-Incremental Learning

- Class Incremental Learning (CIL)을 위한 모델을 제안했다.
- 점점 classes의 개수가 증가함
- 새로운 데이터를 학습할 수 있는 plasticity 능력도 있어야 하고, 기존의 데이터를 잊지 않는 stability 능력도 있어야 한다.
- 두  blocks을 제안함 : a stable block and a plastic block

### AANets (Adaptive Aggrefation Networks)
![image](https://user-images.githubusercontent.com/70581043/177259078-cdd3dae0-edae-4fc7-b545-4f134eb84461.png)
- based on ResNet
- two residual blocks : one for maintaining the knowledge of old classes (i.e., the stabil-ity) and the other for learning new classes (i.e., the plastic-ity)
![image](https://user-images.githubusercontent.com/70581043/177259251-b2c79036-3b78-4dfb-b793-a58172f4ec83.png)
- orange block : 새로운 데이터를 학습
- blue block : 기존의 classes에서 학습한 지식을 고정시킨다.
- two residual blocks에서 학습한 feature map parameters와 aggregation weights를 곱해준다. 
![image](https://user-images.githubusercontent.com/70581043/177259780-e111b5b0-0ee1-4b6c-843c-58b5a4b6707d.png)
- stable block와 plastic block에 weights 곱한 걸 합한다. 
![image](https://user-images.githubusercontent.com/70581043/177260168-d8d2d2b5-a73d-482c-ad5d-0d661c1e4e55.png)

### Optimization Steps
- bilevel optimization program (BOP) : to learn the optimal alpha and [pi, n] that minimize the classification loss on all training samples seen 
![image](https://user-images.githubusercontent.com/70581043/177261196-9045e29a-08f8-44d6-905a-c2e0a597b1f4.png)
이때 loss objectives 는 cross-entropy loss를 사용함
근데 여기서 D를 구할 수 없다. 우리가 구할 수 있는 건  small set of examples ![image](https://user-images.githubusercontent.com/70581043/177261860-af7893f2-1332-466f-9fb0-cd2b78e9c9ae.png) 구할 수 있음.
forgetting problems를 해결하기 위해서, 위의 식을 아래처럼 변형
- 1) **in the upper-level problem** : alpha_i is used to balance the stable and the plastic blocks, so we use the balanced subset to update
it, i.e., learning alpha_i on ![image](https://user-images.githubusercontent.com/70581043/177262203-fdb054e5-75aa-4034-955d-c74781788ebe.png)  adaptively
- 2) **in the lower-level problem**, ![image](https://user-images.githubusercontent.com/70581043/177262293-e34cb0f5-37c2-4d86-8d17-8beb05c883b7.png) are the network parameters used for feature extraction, so we leverage all the available data to train them, i.e., base-training  ![image](https://user-images.githubusercontent.com/70581043/177262293-e34cb0f5-37c2-4d86-8d17-8beb05c883b7.png) on ![image](https://user-images.githubusercontent.com/70581043/177262490-68a2f613-03ab-44f8-be68-c090b348eecf.png)
![image](https://user-images.githubusercontent.com/70581043/177262520-d39c56c1-848d-4316-baf6-da39fcb6de07.png)
