# PTR: Prompt Tuning with Rules for Text Classification

text classification task를 위해 logic rules를 활용한 prompt tuning 방법을 제안했다.

![image](https://user-images.githubusercontent.com/70581043/175010660-440a45d0-aea6-4e63-a1d0-8b61dc75300f.png)
그동안의 NLP training 방법을 살펴보면, Pre-training할 때는 MASK token 부분에 어떤 단어가 들어갈 지 예측하는 MLM 모델로 훈련을 시켰다. 하지만 막상 fine-tuning할 때는 CLS 부분에서 classification/generation/ sequence labeling 등 pre-training 했을 때와는 다른 task를 수행하게 된다. 이로 인해 gap이 생겼음     


이를 해결하기 위해 prompt learning이 생겼다. Input으로 기존의 문장을 그대로 두고, 특정한 위치에 template을 만들어서 합쳐준다. template에는 [MASK] token이 있어 MASK에 이 문장에 해당하는 class가 들어갈 수 있도록 훈련시킨다. prompt learning을 활용함으로써 train할 때와 fine-tune할 때 동일한 task를 수행하는 모델을 생성할 수 있게 됨    

(PTR은 잘 이해가 안갔습니다 ㅠㅠ... NLP 어렵네요...)

## Prompting Tuning with Rules (PTR)
![image](https://user-images.githubusercontent.com/70581043/175011564-fb4f04c1-c72f-4de3-96b7-51fcf63f72c1.png)
![image](https://user-images.githubusercontent.com/70581043/175012246-d7e71786-8419-41ab-8f15-878c68a8f540.png)
- 위의 logic tyles처럼 특정한 논리구조로 prompt를 만든다.
- Composing Sub-Prompts : ![image](https://user-images.githubusercontent.com/70581043/175012892-a421e96e-42a3-48a3-af28-a3228a5f7a39.png)
- prediction probability : ![image](https://user-images.githubusercontent.com/70581043/175013001-e144a236-5a17-4487-88c7-6b98f72b61a2.png)
- final learning objective of PTR is to maximize : ![image](https://user-images.githubusercontent.com/70581043/175013049-debb95fb-5292-421c-ae77-57b0339c1c19.png)
