# ALBEF (ALign the image and text representations BEfore Fusing)

- Large scale vision and language representation learning
- 기존 VLP framework의 한계점
    1. vision과 language 사이의 embedding space가 공유되지 않았다. 따라서 두 modal을 interaction하는 데 한계가 있었음
    2. annotation-expensive and compute expensive
    3. imate-text dataset 매우 noisy 함 

### Acrchitecture
![image](https://user-images.githubusercontent.com/70581043/162720834-23bbdd65-748f-426d-94a9-d834da2603c9.png)
#### Model Architecture
- image encoder : 12-layer ViT-B/16
- text encoder : 6-layer BERT base
- multimodal encoder : 6-layer BERT base

#### Pre-training Objectives
- image-text contrastive learning (ITC) : to learn better unimodal representations before fusion
![image](https://user-images.githubusercontent.com/70581043/162721463-bff3d5bd-10c8-4231-92df-3b4469280066.png)

- masked language modeling (MLM) : utilizes both the image and the contextual text to predict the masked words. MLM minimizes a cross-entropy loss
![image](https://user-images.githubusercontent.com/70581043/162721667-1204f7f8-a3b8-441c-9dab-de863fc92729.png)

- image-text matching (ITM) :  whether a pair of image and text is positive (matched) or negative (not matched).
![image](https://user-images.githubusercontent.com/70581043/162721774-476eccbb-f5b6-44f4-afa4-d8f4e5cc4570.png)

#### Momentum Distillation
- propose to learn from pseudo-targets generated by the momentum model
- momentum model : a continuously-evolving teacher which consists of exponential-moving-average versions of the unimodal and multimodal encoders. 
- ITC : ![image](https://user-images.githubusercontent.com/70581043/162723165-cc8ce4e7-1e6d-4a44-af1a-ca1133472b37.png)
- MLM : ![image](https://user-images.githubusercontent.com/70581043/162723192-56cb2c49-380a-43d6-a96b-a79e69924f4b.png)

### A Mutual Information Maximization Perspective
using "minimizing the InfoNCEloss"
![image](https://user-images.githubusercontent.com/70581043/162723328-ce41c481-3a08-440f-8231-2546390ac1fc.png)

- ITC loss : ![image](https://user-images.githubusercontent.com/70581043/162723362-87a118ae-1fb8-4cfc-8e5d-dcab53f8de45.png)
- MLM loss : ![image](https://user-images.githubusercontent.com/70581043/162723399-47e64bc3-f7ab-48a4-9445-60441f66605e.png)

## Results
![image](https://user-images.githubusercontent.com/70581043/162723492-7648fb3f-2859-4b99-afc9-b38d8e1a02ca.png)
- Evaluation on the Proposed Methods : MLM, ITM, ITC, MoD 모든 걸 넣었을 때 성능이 가장 좋았다.
![image](https://user-images.githubusercontent.com/70581043/162723634-4140821e-5616-4612-8841-49f7603844c8.png)
- Evaluation on Image-Text Retrieval : training image가 4M 일 때보다 14M일 때 성능이 더 좋았다.
![image](https://user-images.githubusercontent.com/70581043/162723744-5d3d1633-13ef-48d4-8a3a-bb8153b0f7c1.png)
- Evaluation on VQA, NLVR, and VE : ALBEF 성능 좋음, 게다가 detector도 없고, lower resolution 사용해도 되기 때문에 훨씬 더 빠른 inference 가 가능함
![image](https://user-images.githubusercontent.com/70581043/162723987-99c42798-37ef-4b5b-a06f-2b789fc7fdb4.png)
- Weakly-supervised Visual Grounding : Grad-CAM visualizations from ALBEF are highly correlated with where humans would look when making decisions. 
