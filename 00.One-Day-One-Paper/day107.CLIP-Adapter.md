CLIP성능을 높이는 방법으로 CLIP-Adapter 모델을 제안함,    
CoOp처럼 prompt engineering을 하는 것이 아니라, additional bottleneck layer을 넣고 굉장히 light하게 fine tuning을 함으로써 성능을 높일 수 있음을 제안했다.     


## CLIP-Adapter
![image](https://user-images.githubusercontent.com/70581043/185058988-88b080d2-9f5d-408b-8e94-280ed0a407de.png)
- only finetunes a small number of additional weights instead of optimizing all parameter of CLIP
- adopts a lightweight bottleneck architecture (adds two additional linear layers)
- residual-style blending : 기존의 f(image)와 finetuning feature를 합치는 구간

### ClassifierWeight Generation for Few-Shot Learning
![image](https://user-images.githubusercontent.com/70581043/185059870-14bd82f7-d153-4e52-b650-4215e6027d1a.png)
f : 인풋 이미지를 model에 넣어서 feature vector를 얻는다.
p : softmax function을 사용. category i의 확률을 나타냄
![image](https://user-images.githubusercontent.com/70581043/185060189-b571e68e-679c-4080-8502-80793995d357.png)
CLIP에서는 H라는 prompt와 C_i라는 class name을 합쳐서 Tokenizer 시키고 BERT 모델을 거친다. (해당 논문도 동일한 방식으로 진행)

### CLIP-Adapter
![image](https://user-images.githubusercontent.com/70581043/185060436-090ae1a6-3bfa-4615-964f-541b5539a455.png)
- image feautre (f) -> learnable feature A_v를 통과함 (두 개의 linear layer로 구성됨)
- classifier weight (W) -> learnable feature A_t를 통과함 (두 개의 linear layer로 구성됨)
![image](https://user-images.githubusercontent.com/70581043/185060665-bb94fe5b-c5cb-43b6-bd4b-8fc955de48a9.png)
- residual connection : 기존의 정보를 잊혀지는 것을 방지하기 위해 사용함. 알파와 베타는 각각 "residudal ratio"
    - residual ratio 높게 설정 : 기존의 데이터베이스와 새로운 데이터베이스가 차이가 많이 날 때
    - residual ratio 낮게 설정 : 기존의 데이터베이스와 새로운 데이터베이스가 유사할 때
- 새롭게 얻은 f*와 W*를 (1)번 식에 넣어 probability를 구한다.
![image](https://user-images.githubusercontent.com/70581043/185061105-0b98b17e-e224-4d4f-a9ed-ea783926d29e.png)
cross-entropy loss로 optimaize 시킴

## Results
- 11개의 데이터베이스로 모델 성능 비교함            
- 1) Zero-shot CLIP, 2) Linear probe CLIP, 3) CoOp 간 성능 비교함
![image](https://user-images.githubusercontent.com/70581043/185061345-a16ea336-b538-49d3-90df-db3d74597d8e.png)
CLIP-Adapter가 1) Zero-shot CLIP, 2) Linear probe CLIP, 3) CoOp  보다 거의 다 좋은 성능을 보여줌
![image](https://user-images.githubusercontent.com/70581043/185061626-285ae97f-aeab-4e3a-8d45-a3dbc36d44de.png)
t-SNE를 봤을 때도 CLIP-Adapter가 동일한 class끼리 더 잘 구분해주고 있다.