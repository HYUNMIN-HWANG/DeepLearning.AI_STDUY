# Deep-learning-based automated terminology mapping in OMOP-CDM
병원마다 환자 데이터를 저장하는 게 다르고, 국가마다 저장하는 언어도 다르다. 이를 OMOP-CDM에서 표준화하여 저장하고 있다. 해당 논문에서는 deep learning 기법을 사용하여 한국의 5개 상급 병원 데이터를 OMOP-CDM에 매핑하는 방식을 제안했다. 

### Usagi
- Usagi is an open-source software used to perform automatic vocabulary mapping.
- term frequency-inverse document frequency (TF-IDF)-based similarity computation 방식을 사용함 : TF 에서는 단어가 나타나는 빈도수를 계산하고, IDF에서는 단어의 uniqueness를 계산한다. 
- 각 단어들의 TF-IDF를 계산해서 가장 높은 cosine similarity를 나타내는 vector값을 찾는다.
- 한계점 : it does not incorporate many useful types of textual information, such as word co-occurrence statistics, positions, and semantics.

## Data
- source (S): 한국에 있는 5개 상급병원
- target (T) : SNOMED clinical terms code, English CDM
- 하나의 source 개념이 여러 개 target 의미를 가질 수 있음 (one-to-many relationship)
![image](https://user-images.githubusercontent.com/70581043/182344512-fcc83231-ed7c-4910-9af4-5655486e8d15.png)
t^S : source data의 code         
d^S : source data의 description sentences       
t^T : target data의 code        
d^T : target data의  description sentences          
y : label (1 : mapping이 올바른 경우 positive sample, 0 : mapping이 잘못된 경우 negative sample)         
   
S와 T 간의 **Eulidean embedding** 거리를 구한다. 비슷한 의미의 vector끼리는 가깝게, 서로 다른 의미 vector끼리는 멀리 매칭한다.

### negative sample 만드는 3가지 방식
![image](https://user-images.githubusercontent.com/70581043/182345127-41929723-c1cf-4d88-97a4-1f1facf053bf.png)
1. random sampling scheme : code와 description을 랜덤하게 매칭시킨다.
2. False-positive Sampling Scheme : pre-trained model에서 나온 score를 기준으로 top 100개 리스트를 뽑고 target name이 아닌 경우 
3. Hierarchical Sampling Scheme : 수직적 관계를 만들어 target code보다 위 아래로 2개 이상 떨어진 경우

## Text-based OMOP Knowledge Integration(TOKI)
### Learning architecture
![image](https://user-images.githubusercontent.com/70581043/182345626-fe71a14b-8c07-4d85-85b7-828ada847aa5.png)
- source data와 target data를 각기 따로 입력을 시킨 후, 각각 embedding vector를 거치고, RNN 모델을 통과시킨다.
- 각 u와 v 벡터로 similarity를 계산한다.
   - (u, v, |u-v|, u ∗ v) 4가지 방법으로 계산
- 2개의 FC 계층으로 합쳐 probability를 계산한다.
   - ![image](https://user-images.githubusercontent.com/70581043/182346271-6847e243-ebf6-49a0-8ef4-9e18287a4634.png)
   - 처음에는 ReLU 계층을 거쳐 4d vector를 512 dimension representation으로 바꾼다.
   - 그 후, sigmoid를 거쳐 probability를 계산함

### Training
- 목적 : maximizing the probabilties of the positive samples, minimizing the probabilities of the negative samples.
- binary cross-entropy loss function
![image](https://user-images.githubusercontent.com/70581043/182346780-7165691e-0438-40a9-9754-1e682547df68.png)

### Test procedure
- top-K accuracy metric
![image](https://user-images.githubusercontent.com/70581043/182346890-033c19a4-998c-435a-bb0e-8926203c36db.png)

### Results
![image](https://user-images.githubusercontent.com/70581043/182346955-5796bb0b-ba71-407b-bcde-13a9abff9961.png)
- Usagi와 비교했을 때 50개 이상 random sample했을 때 (50 POS) 부터 모든 TOKI 성능이 더 좋았다.
- Top-k에서 k 개수가 증가할 수록 결과가 좋았다.
- FP도 함께 했을 때, HIER까지 함께 했을 때 더 결과가 좋았다. -> 다양한 negative sampling 방식을 적용했을 때 성능이 더 좋아진다.
