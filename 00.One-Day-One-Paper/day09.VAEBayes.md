# Auto-Encoding Variational Bayes
- https://arxiv.org/abs/1312.6114
- [Submitted on 20 Dec 2013 (v1), last revised 1 May 2014 (this version, v10)]

---

## Abstract
We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case
Our contributions
1) we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods.
2) we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.

## 1 Introduction
AEVB algorithm

## 2 Method
a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables.
- we have an i.i.d. dataset with latent variables per datapoint
- perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables.

### 2.1 Problem scenario
- some dataset X
- the data are generated by some random process, involving an unobserved continuous random variable z.
- The process consists of two steps:
1. a value z(i) is generated from some prior distribution p (z)
2. value x(i) is generated from some conditional distribution p (x|z).
- Unfortunately, a lot of this process is hidden from our view: the true parameters  as well as the values of the latent variables z(i) are unknown to us.

- we do not make the common simplifying assumptions about the marginal or posterior probabilities.
- we are here interested in a general algorithm that even works efficiently in the case of: Intractability(미분을 할 수 없음), A large dataset

propose a solution
1. parameters theta : mimic the hidden random process and generate artificial data that resembles the real data
2. latent variable z : given an observed value x for a choice of parameters theta.
3. variable x : This allows us to perform all kinds of inference tasks where a prior over x is required

- q(z|x): an approximation to the intractable true posterior p(z|x) == probabilistic encoder,
- p(xjz) as a probabilistic decoder

### 2.2 The variational bound
The marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints 
L(;; x(i)) is called the (variational) lower bound on the marginal likelihood of datapoint i

### 2.3 The SGVB estimator and AEVB algorithm
The first term is (the KL divergence of the approximate posterior from the prior) acts as a regularizer, while the second term is a an expected negative reconstruction error.

### 2.4 The reparameterization trick
z be a continuous random variable
Z ~  q(zjx) be some conditional distribution.
we applied this trick to obtain a differentiable estimator of the variational lower bound.

## 3 Example: Variational Auto-Encoder

## 4 Related work

## 5 Experiments
The generative model (encoder) and variational approximation (decoder) 
the described encoder and decoder have an equal number of hidden units.

## 6 Conclusion
We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables

---
---
---

# Auto-Encoding Variational Bayes
- https://arxiv.org/abs/1312.6114
- [Submitted on 20 Dec 2013 (v1), last revised 1 May 2014 (this version, v10)]

---

## Summary
**Auto-Encoding VB (AEVB) algorithm**
미분할 수 없는 intractable posterior distributions 혹은 large datasets의 경우 효율적인 probabilistic 모델을 제안한다.
-  reparameterization 방법을 사용해서 미분할 수 있다. (reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound)
- approximate inference model 을 사용한다.

## Problem scenario
![image](https://user-images.githubusercontent.com/70581043/128625655-47c498ad-a319-4a9b-bda4-d98328e0dee4.png)
훈련할 때의 문제는  true parameter *theta*와 latent variables *z*를 모른다는 것이다. 이를 해결하기 위해서 3가지를 제안한다.

1. parameter *theta* : hidden random process를 흉내내어 실제 데이터와 비슷한 가상의 데이터를 만들 수 있다.
2. variable *z* : parameter *theta*가 주어졌을 때 관찰되는 *x* 값과 유사한 latent variable
3. variable *x* :  이미지 디노이즈, super-resolution에 활용될 수 있다.

위 문제를 풀기 위해서 실제 값인 ![image](https://user-images.githubusercontent.com/70581043/128625906-9f35a63b-590a-4dfb-86b7-372aa52ae42a.png) 과 근사한 ![image](https://user-images.githubusercontent.com/70581043/128625901-a8b181a2-eea7-4df5-b83c-2d72e8f6a358.png) 을 제안함
- ![image](https://user-images.githubusercontent.com/70581043/128625937-e9aae80e-4285-4ecf-9033-1b3d0c463865.png) : probabilistic encoder
- ![image](https://user-images.githubusercontent.com/70581043/128625950-03897126-e109-4feb-bf90-f5235d49f40e.png) : probabilistic decoder

## The variational bound
![image](https://user-images.githubusercontent.com/70581043/128625987-37e61ee7-f1d3-4085-8368-e3605b7d914e.png)
모든 p_theta(x i) 의 확률은 ① 실제 값과 얼마나 근사한지 값 ② 가장 최저의 한계 가능성을 나타낸다. 

## AEVB algorithm
- ![image](https://user-images.githubusercontent.com/70581043/128626112-93f10d16-7e4d-483a-88d0-279d0ebc03a0.png)
![image](https://user-images.githubusercontent.com/70581043/128626121-7befa035-e4dd-418a-ba86-9d8d6db14ea3.png) 을 미분할 수 있는 변형된 형태
- generic Stochastic Gradient Variational Bayes (SGVB) estimator
![image](https://user-images.githubusercontent.com/70581043/128626226-1b4eb37f-e6af-4ca0-8220-bf0f1cfa4cc5.png)
- second version of the SGVB estimator
![image](https://user-images.githubusercontent.com/70581043/128626247-41aa6bde-823e-44d4-bd0e-be56f6110f5f.png)
> - 첫번째 수식(KL divergence) : regularizer 같은 역할을 수행함
> - 두번째 수식 : negative reconstruction error
- multiple datapoints from a dataset X with N datapoints
![image](https://user-images.githubusercontent.com/70581043/128626258-2a94d3fd-54cc-46fd-9ec3-76f5e5158253.png)

## The reparameterization trick
variational lower bound 의 미분값을 얻을 수 있는데 사용할 수 있다.

---

뭐라고 하는건지 모르겠,,,,, 추가적으로 다른 참고자료들을 보면서 이해해야겠습니다..😭

---
요약하자면, 우리가 구하고 싶은 것은 x에 대한 확률 값이다. ![image](https://user-images.githubusercontent.com/70581043/128833399-8fcef3b6-9a8b-49c6-b30c-4da331633ae5.png) 이를 구하기 위해서 DKL의 최소값을 구해야 하는 문제가 된다. 이는 곧 Lower bound의 최댓값을 구하는 문제와 동일하다. 우리는 실제 x값과 z값을 모르기 때문에 실제 값과 근사한 ![image](https://user-images.githubusercontent.com/70581043/128833827-4cc847be-6e9d-4e9d-afb6-643712670df0.png) 을 설정해준다. 또한 미분이 가능하도록 reparameterization trick을 사용했다. 즉 z를 함수 g로 표현함으로써 (평균, 분산)에 대해 미분이 가능하게 바꾸었다. 최종 loss function은 다음과 같다 
![image](https://user-images.githubusercontent.com/70581043/128834225-8fc5b457-f3a4-408f-b044-f7e836d9b231.png)


---
---
---
[추가 정리]

## 기본 개념
**1. 오토인코더**
![image](https://user-images.githubusercontent.com/70581043/128823841-efdd8a56-443b-4791-99a0-aedbd07bec26.png)
- 오토인코더는 고차원의 입력 데이터(x)를 저차원의 잠재변수(latent variables; z)로 압축(=차원축소)한 뒤, 다시 입력 데이터에 가까운 고차원 데이터(x’)로 복원하는 구조의 모델 
- 압축하는 부분을 encoder, 복원하는 부분을 decoder
- encoder를 하면서 데이터가 가진 수십, 수백개 변수로부터 중요한 변수를 추출함
- decoder는 정답 데이터를 입력 데이터로 사용함 ( Self-supervised learning )
- VAE에서는 decoder를 사용해서 새 데이터를 만들어 내기 때문에 생성 모델의 용도로 사용됨

**2. Variational inference**
![image](https://user-images.githubusercontent.com/70581043/128824437-26a0f228-a178-4303-8f6c-8a1e60459f05.png)
위의 식을 최소하하면,
![image](https://user-images.githubusercontent.com/70581043/128824519-6743ace8-b8b0-4c1e-82c2-d268f3b31e0b.png)
![image](https://user-images.githubusercontent.com/70581043/128824711-b98cfb47-b54b-4c5d-a13b-e801de4dd5b6.png)

- Variational inference는, 어떤 사후확률 을 알고 싶을 때 이를 근사(approximate)한 확률분포를 상정한 뒤, 두 분포 간 차이를 수치화한 KL divergence를 최소화해 근사 분포를 구하는 접근법
-  ‘Lower bound’(또는 ‘ELBO’)를 최대화하는 문제로 구해도 된다.

**3. Variational 오토인코더(VAE)**
![image](https://user-images.githubusercontent.com/70581043/128825034-03cf1d47-be22-4b55-8b53-2edc9ab301c0.png)
- VAE는 2번의 Variational inference를 사용한 오토인코더
- x를 z의 approximation 분포의 평균, 분산 벡터로 인코딩
손실함수
![image](https://user-images.githubusercontent.com/70581043/128825500-ee46ae27-5f22-4a61-a565-7dcaeaee1cc7.png)

4. 
## 1. Problem Suggestion 
![image](https://user-images.githubusercontent.com/70581043/128815836-96290485-a175-40a9-9d16-5ab18ccd697e.png)
VAE에서의 목표는 posterior distribution 분포를 알아내기 어려운 문제를 임시 파라미터를 두어 학습하는 것이다. 위 그림에서 우리는  pθ(x)가 무엇인지 모르기 때문에  pθ(z|x)을 알아내는 것이 불가능하다. 따라서 이에 근사한 값인 qφ(z|x)(검정색 점선)을 두어 posterior를 대신 모델링 하도록 한다.


## 2. 1 Problem scenario
- 데이터셋이 생성되는 과정은 아래와 같다.

> 1. 잠재변수 z가  Prior distribution pθ*로부터 생성된다.
> 2.  dataset xi 가 어떤 conditional distribution pθ*(x|z)로부터 생성된다.     
          
- 하지만 문제는 parameter θ*를 모르고, latent variable z도 모른다. (모델링이 어렵다.)         

- marginal probability p(x) 를 쉽게 이용하기 위한 가정은 하지 않았다.
> -  marginal likelihood를 구할 수 없는 경우도 동작 가능
> - 데이터 셋이 큰 경우도 동작 가능    

![image](https://user-images.githubusercontent.com/70581043/128817604-cad3a0ed-31ef-4f86-9a0f-ba3da4e569bf.png)
- recognition model qφ(z|x)를 도입함   
> - true posterior pθ(z|x)에 대한 approximation
> - 인코더 라고 볼 수 도 있다.

## 2.2 Variational Bound
![image](https://user-images.githubusercontent.com/70581043/128820419-eb8ac191-704f-489e-9239-f0e9d25701a5.png)
![image](https://user-images.githubusercontent.com/70581043/128826409-8143d5c5-dfa2-4513-ad39-694e8c3c2517.png)

x에 관한 marginal likelihood 를 위와 같이 정의할 수 있다. 이때 DKL값이 항상 non-negative이기 때문에 해당 식은 아래와 같이 쓸 수도 있다.
![image](https://user-images.githubusercontent.com/70581043/128820558-69257ca9-ca5c-4801-8bd0-174f4fe7a891.png)
우변을 "variational lower bound" 라고 부른다. 이때  varaiational parameter 인 φ와 generative parameter인  θ 을 모두 최적화하고 싶다.

## 2.3 The SGVB estimator and AEVB algorithm
posterior를 다음과 같이 reparameterize 한다. z ∼ qφ(z|x)
![image](https://user-images.githubusercontent.com/70581043/128820927-0ca44b9d-3823-4abf-921f-54f3f3c642d4.png)
variational lower bound 첫 번째 식(일반적인, Stochastic Gradient Variational Bayes estimator)
![image](https://user-images.githubusercontent.com/70581043/128821673-96adab74-bf91-4fb4-95a7-a51db3959023.png)
variational lower bound 두 번째 식( kl divergence는  φ를 regularization 하는 term)
![image](https://user-images.githubusercontent.com/70581043/128821755-816ac8b5-7fa4-4f45-b68c-fb0547464a51.png)
전체 dataset 에 대한 Lower bound
![image](https://user-images.githubusercontent.com/70581043/128822010-4525c6f2-4133-4762-b9ef-f0aa2e56ec39.png)

## 2.4 The reparametrization trick 
![image](https://user-images.githubusercontent.com/70581043/128825586-a42a95a5-e1fe-42b4-976d-767de77908ee.png)
z를 함수 g로 표현함으로써 (평균, 분산)에 대해 미분이 가능하게 바꾸는 트릭

## 3. Example : Variational Auto-Encoder 
 loss function식
![image](https://user-images.githubusercontent.com/70581043/128822333-1a75fc68-ea4e-49f1-96d3-7eee281392d7.png)

---
[참고자료] 
- https://judy-son.tistory.com/11
- https://garden-k.medium.com/testing-b3966b642cf5

---
