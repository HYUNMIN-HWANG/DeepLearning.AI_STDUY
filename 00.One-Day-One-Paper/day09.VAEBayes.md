# Auto-Encoding Variational Bayes
- https://arxiv.org/abs/1312.6114
- [Submitted on 20 Dec 2013 (v1), last revised 1 May 2014 (this version, v10)]

---

## Abstract
We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case
Our contributions
1) we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods.
2) we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.

## 1 Introduction
AEVB algorithm

## 2 Method
a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables.
- we have an i.i.d. dataset with latent variables per datapoint
- perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables.

### 2.1 Problem scenario
- some dataset X
- the data are generated by some random process, involving an unobserved continuous random variable z.
- The process consists of two steps:
1. a value z(i) is generated from some prior distribution p (z)
2. value x(i) is generated from some conditional distribution p (x|z).
- Unfortunately, a lot of this process is hidden from our view: the true parameters  as well as the values of the latent variables z(i) are unknown to us.

- we do not make the common simplifying assumptions about the marginal or posterior probabilities.
- we are here interested in a general algorithm that even works efficiently in the case of: Intractability(ë¯¸ë¶„ì„ í•  ìˆ˜ ì—†ìŒ), A large dataset

propose a solution
1. parameters theta : mimic the hidden random process and generate artificial data that resembles the real data
2. latent variable z : given an observed value x for a choice of parameters theta.
3. variable x : This allows us to perform all kinds of inference tasks where a prior over x is required

- q(z|x): an approximation to the intractable true posterior p(z|x) == probabilistic encoder,
- p(xjz) as a probabilistic decoder

### 2.2 The variational bound
The marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints 
L(;; x(i)) is called the (variational) lower bound on the marginal likelihood of datapoint i

### 2.3 The SGVB estimator and AEVB algorithm
The first term is (the KL divergence of the approximate posterior from the prior) acts as a regularizer, while the second term is a an expected negative reconstruction error.

### 2.4 The reparameterization trick
z be a continuous random variable
Z ~  q(zjx) be some conditional distribution.
we applied this trick to obtain a differentiable estimator of the variational lower bound.

## 3 Example: Variational Auto-Encoder

## 4 Related work

## 5 Experiments
The generative model (encoder) and variational approximation (decoder) 
the described encoder and decoder have an equal number of hidden units.

## 6 Conclusion
We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables

---
---
---

# Auto-Encoding Variational Bayes
- https://arxiv.org/abs/1312.6114
- [Submitted on 20 Dec 2013 (v1), last revised 1 May 2014 (this version, v10)]

---

## Summary
**Auto-Encoding VB (AEVB) algorithm**
ë¯¸ë¶„í•  ìˆ˜ ì—†ëŠ” intractable posterior distributions í˜¹ì€ large datasetsì˜ ê²½ìš° íš¨ìœ¨ì ì¸ probabilistic ëª¨ë¸ì„ ì œì•ˆí•œë‹¤.
-  reparameterization ë°©ë²•ì„ ì‚¬ìš©í•´ì„œ ë¯¸ë¶„í•  ìˆ˜ ìˆë‹¤. (reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound)
- approximate inference model ì„ ì‚¬ìš©í•œë‹¤.

## Problem scenario
![image](https://user-images.githubusercontent.com/70581043/128625655-47c498ad-a319-4a9b-bda4-d98328e0dee4.png)
í›ˆë ¨í•  ë•Œì˜ ë¬¸ì œëŠ”  true parameter *theta*ì™€ latent variables *z*ë¥¼ ëª¨ë¥¸ë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ 3ê°€ì§€ë¥¼ ì œì•ˆí•œë‹¤.

1. parameter *theta* : hidden random processë¥¼ í‰ë‚´ë‚´ì–´ ì‹¤ì œ ë°ì´í„°ì™€ ë¹„ìŠ·í•œ ê°€ìƒì˜ ë°ì´í„°ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.
2. variable *z* : parameter *theta*ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ê´€ì°°ë˜ëŠ” *x* ê°’ê³¼ ìœ ì‚¬í•œ latent variable
3. variable *x* :  ì´ë¯¸ì§€ ë””ë…¸ì´ì¦ˆ, super-resolutionì— í™œìš©ë  ìˆ˜ ìˆë‹¤.

ìœ„ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ì„œ ì‹¤ì œ ê°’ì¸ ![image](https://user-images.githubusercontent.com/70581043/128625906-9f35a63b-590a-4dfb-86b7-372aa52ae42a.png) ê³¼ ê·¼ì‚¬í•œ ![image](https://user-images.githubusercontent.com/70581043/128625901-a8b181a2-eea7-4df5-b83c-2d72e8f6a358.png) ì„ ì œì•ˆí•¨
- ![image](https://user-images.githubusercontent.com/70581043/128625937-e9aae80e-4285-4ecf-9033-1b3d0c463865.png) : probabilistic encoder
- ![image](https://user-images.githubusercontent.com/70581043/128625950-03897126-e109-4feb-bf90-f5235d49f40e.png) : probabilistic decoder

## The variational bound
![image](https://user-images.githubusercontent.com/70581043/128625987-37e61ee7-f1d3-4085-8368-e3605b7d914e.png)
ëª¨ë“  p_theta(x i) ì˜ í™•ë¥ ì€ â‘  ì‹¤ì œ ê°’ê³¼ ì–¼ë§ˆë‚˜ ê·¼ì‚¬í•œì§€ ê°’ â‘¡ ê°€ì¥ ìµœì €ì˜ í•œê³„ ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤. 

## AEVB algorithm
- ![image](https://user-images.githubusercontent.com/70581043/128626112-93f10d16-7e4d-483a-88d0-279d0ebc03a0.png)
![image](https://user-images.githubusercontent.com/70581043/128626121-7befa035-e4dd-418a-ba86-9d8d6db14ea3.png) ì„ ë¯¸ë¶„í•  ìˆ˜ ìˆëŠ” ë³€í˜•ëœ í˜•íƒœ
- generic Stochastic Gradient Variational Bayes (SGVB) estimator
![image](https://user-images.githubusercontent.com/70581043/128626226-1b4eb37f-e6af-4ca0-8220-bf0f1cfa4cc5.png)
- second version of the SGVB estimator
![image](https://user-images.githubusercontent.com/70581043/128626247-41aa6bde-823e-44d4-bd0e-be56f6110f5f.png)
> - ì²«ë²ˆì§¸ ìˆ˜ì‹(KL divergence) : regularizer ê°™ì€ ì—­í• ì„ ìˆ˜í–‰í•¨
> - ë‘ë²ˆì§¸ ìˆ˜ì‹ : negative reconstruction error
- multiple datapoints from a dataset X with N datapoints
![image](https://user-images.githubusercontent.com/70581043/128626258-2a94d3fd-54cc-46fd-9ec3-76f5e5158253.png)

## The reparameterization trick
variational lower bound ì˜ ë¯¸ë¶„ê°’ì„ ì–»ì„ ìˆ˜ ìˆëŠ”ë° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

---

ë­ë¼ê³  í•˜ëŠ”ê±´ì§€ ëª¨ë¥´ê² ,,,,, ì¶”ê°€ì ìœ¼ë¡œ ë‹¤ë¥¸ ì°¸ê³ ìë£Œë“¤ì„ ë³´ë©´ì„œ ì´í•´í•´ì•¼ê² ìŠµë‹ˆë‹¤..ğŸ˜­


---

