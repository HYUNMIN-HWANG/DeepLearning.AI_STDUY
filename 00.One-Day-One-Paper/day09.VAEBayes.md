# Auto-Encoding Variational Bayes
- https://arxiv.org/abs/1312.6114
- [Submitted on 20 Dec 2013 (v1), last revised 1 May 2014 (this version, v10)]

---

## Abstract
We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case
Our contributions
1) we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods.
2) we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.

## 1 Introduction
AEVB algorithm

## 2 Method
a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables.
- we have an i.i.d. dataset with latent variables per datapoint
- perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables.

### 2.1 Problem scenario
- some dataset X
- the data are generated by some random process, involving an unobserved continuous random variable z.
- The process consists of two steps:
1. a value z(i) is generated from some prior distribution p (z)
2. value x(i) is generated from some conditional distribution p (x|z).
- Unfortunately, a lot of this process is hidden from our view: the true parameters  as well as the values of the latent variables z(i) are unknown to us.

- we do not make the common simplifying assumptions about the marginal or posterior probabilities.
- we are here interested in a general algorithm that even works efficiently in the case of: Intractability(미분을 할 수 없음), A large dataset

propose a solution
1. parameters theta : mimic the hidden random process and generate artificial data that resembles the real data
2. latent variable z : given an observed value x for a choice of parameters theta.
3. variable x : This allows us to perform all kinds of inference tasks where a prior over x is required

- q(z|x): an approximation to the intractable true posterior p(z|x) == probabilistic encoder,
- p(xjz) as a probabilistic decoder

### 2.2 The variational bound
The marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints 
L(;; x(i)) is called the (variational) lower bound on the marginal likelihood of datapoint i

### 2.3 The SGVB estimator and AEVB algorithm
The first term is (the KL divergence of the approximate posterior from the prior) acts as a regularizer, while the second term is a an expected negative reconstruction error.

### 2.4 The reparameterization trick
z be a continuous random variable
Z ~  q(zjx) be some conditional distribution.
we applied this trick to obtain a differentiable estimator of the variational lower bound.

## 3 Example: Variational Auto-Encoder

## 4 Related work

## 5 Experiments
The generative model (encoder) and variational approximation (decoder) 
the described encoder and decoder have an equal number of hidden units.

## 6 Conclusion
We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables

---
---
---

# Auto-Encoding Variational Bayes
- https://arxiv.org/abs/1312.6114
- [Submitted on 20 Dec 2013 (v1), last revised 1 May 2014 (this version, v10)]

---

## Summary
**Auto-Encoding VB (AEVB) algorithm**
미분할 수 없는 intractable posterior distributions 혹은 large datasets의 경우 효율적인 probabilistic 모델을 제안한다.
-  reparameterization 방법을 사용해서 미분할 수 있다. (reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound)
- approximate inference model 을 사용한다.

## Problem scenario
![image](https://user-images.githubusercontent.com/70581043/128625655-47c498ad-a319-4a9b-bda4-d98328e0dee4.png)
훈련할 때의 문제는  true parameter *theta*와 latent variables *z*를 모른다는 것이다. 이를 해결하기 위해서 3가지를 제안한다.

1. parameter *theta* : hidden random process를 흉내내어 실제 데이터와 비슷한 가상의 데이터를 만들 수 있다.
2. variable *z* : parameter *theta*가 주어졌을 때 관찰되는 *x* 값과 유사한 latent variable
3. variable *x* :  이미지 디노이즈, super-resolution에 활용될 수 있다.

위 문제를 풀기 위해서 실제 값인 ![image](https://user-images.githubusercontent.com/70581043/128625906-9f35a63b-590a-4dfb-86b7-372aa52ae42a.png) 과 근사한 ![image](https://user-images.githubusercontent.com/70581043/128625901-a8b181a2-eea7-4df5-b83c-2d72e8f6a358.png) 을 제안함
- ![image](https://user-images.githubusercontent.com/70581043/128625937-e9aae80e-4285-4ecf-9033-1b3d0c463865.png) : probabilistic encoder
- ![image](https://user-images.githubusercontent.com/70581043/128625950-03897126-e109-4feb-bf90-f5235d49f40e.png) : probabilistic decoder

## The variational bound
![image](https://user-images.githubusercontent.com/70581043/128625987-37e61ee7-f1d3-4085-8368-e3605b7d914e.png)
모든 p_theta(x i) 의 확률은 ① 실제 값과 얼마나 근사한지 값 ② 가장 최저의 한계 가능성을 나타낸다. 

## AEVB algorithm
- ![image](https://user-images.githubusercontent.com/70581043/128626112-93f10d16-7e4d-483a-88d0-279d0ebc03a0.png)
![image](https://user-images.githubusercontent.com/70581043/128626121-7befa035-e4dd-418a-ba86-9d8d6db14ea3.png) 을 미분할 수 있는 변형된 형태
- generic Stochastic Gradient Variational Bayes (SGVB) estimator
![image](https://user-images.githubusercontent.com/70581043/128626226-1b4eb37f-e6af-4ca0-8220-bf0f1cfa4cc5.png)
- second version of the SGVB estimator
![image](https://user-images.githubusercontent.com/70581043/128626247-41aa6bde-823e-44d4-bd0e-be56f6110f5f.png)
> - 첫번째 수식(KL divergence) : regularizer 같은 역할을 수행함
> - 두번째 수식 : negative reconstruction error
- multiple datapoints from a dataset X with N datapoints
![image](https://user-images.githubusercontent.com/70581043/128626258-2a94d3fd-54cc-46fd-9ec3-76f5e5158253.png)

## The reparameterization trick
variational lower bound 의 미분값을 얻을 수 있는데 사용할 수 있다.

---

뭐라고 하는건지 모르겠,,,,, 추가적으로 다른 참고자료들을 보면서 이해해야겠습니다..😭


---

